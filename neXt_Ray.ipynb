{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neXt-Ray",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anfals/neXt-Ray/blob/main/neXt_Ray.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAuDxr7-UJs6"
      },
      "source": [
        "# Imports and Installations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsSJa1Oi7i9l"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.checkpoint as cp\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "from torch.jit.annotations import List\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, cohen_kappa_score, confusion_matrix, roc_curve, auc\n",
        "from enum import Enum\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84IREE81UVOf"
      },
      "source": [
        "# Data Loaders Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hK7RzTHx9lWh"
      },
      "source": [
        "# Data Loaders\n",
        "train_transformer = transforms.Compose([\n",
        "    transforms.Resize(320),  # resize the image to 320x320\n",
        "    transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(), # transform it into a torch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) # normalize to weights from ImageNet\n",
        "\n",
        "# loader for evaluation, no horizontal flip\n",
        "eval_transformer = transforms.Compose([\n",
        "    transforms.Resize(320),  # resize the image to 320x320 \n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.ToTensor(), # transform it into a torch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) # normalize to weights from ImageNet\n",
        "\n",
        "class Region(Enum):\n",
        "    WRIST = 1\n",
        "    SHOULDER = 2\n",
        "    HUMERUS = 3\n",
        "    HAND = 4\n",
        "    FOREARM = 5\n",
        "    FINGER = 6\n",
        "    ELBOW = 7\n",
        "\n",
        "class MuraDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A standard PyTorch definition of Dataset which defines the functions __len__ and __getitem__.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, transform, region=None):\n",
        "        \"\"\"\n",
        "        Store the filenames of the jpgs to use. Specifies transforms to apply on images.\n",
        "\n",
        "        Args:\n",
        "            data_dir: (string) directory containing the dataset\n",
        "            transform: (torchvision.transforms) transformation to apply on image\n",
        "            region: Region to filter the data files by\n",
        "        \"\"\"\n",
        "        self.filenames = os.listdir(data_dir)\n",
        "        if region is not None:\n",
        "            self.filenames = [os.path.join(data_dir, f) for f in self.filenames if f.endswith('.png') and region.name in f]\n",
        "        else:\n",
        "            self.filenames = [os.path.join(data_dir, f) for f in self.filenames if f.endswith('.png')]\n",
        "\n",
        "        self.labels = [int(os.path.split(filename)[-1][0]) for filename in self.filenames]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # return size of dataset\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetch index idx image and labels from dataset. Perform transforms on image.\n",
        "\n",
        "        Args:\n",
        "            idx: (int) index in [0, 1, ..., size_of_dataset-1]\n",
        "\n",
        "        Returns:\n",
        "            image: (Tensor) transformed image\n",
        "            label: (int) corresponding label of image\n",
        "        \"\"\"\n",
        "        image = Image.open(self.filenames[idx])  \n",
        "        image = self.transform(image)\n",
        "        return image, self.labels[idx]\n",
        "\n",
        "\n",
        "def fetch_dataloader(types, data_dir, params):\n",
        "    \"\"\"\n",
        "    Fetches the DataLoader object for each type in types from data_dir.\n",
        "\n",
        "    Args:\n",
        "        types: (list) has one or more of 'train', 'val', 'test' depending on which data is required\n",
        "        data_dir: (string) directory containing the dataset\n",
        "        params: (Params) hyperparameters\n",
        "\n",
        "    Returns:\n",
        "        data: (dict) contains the DataLoader object for each type in types\n",
        "    \"\"\"\n",
        "    dataloaders = {}\n",
        "\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        if split in types:\n",
        "            path = os.path.join(data_dir, \"{}\".format(split))\n",
        "\n",
        "            # use the train_transformer if training data, else use transformer without random flip\n",
        "            if split == 'train':\n",
        "                dl = DataLoader(MuraDataset(path, train_transformer), batch_size=params.batch_size, shuffle=True,\n",
        "                                num_workers=params.num_workers,\n",
        "                                pin_memory=params.cuda)\n",
        "            elif split =='eval':\n",
        "                dl = DataLoader(MuraDataset(path, eval_transformer), batch_size=params.batch_size, shuffle=False,\n",
        "                                num_workers=params.num_workers,\n",
        "                                pin_memory=params.cuda)\n",
        "            else:\n",
        "                for r in Region:\n",
        "                  dl = DataLoader(MuraDataset(path, eval_transformer, r), batch_size=params.batch_size, shuffle=False,\n",
        "                                num_workers=params.num_workers,\n",
        "                                pin_memory=params.cuda)\n",
        "                  dataloaders[r.name] = dl\n",
        "                dl = DataLoader(MuraDataset(path, eval_transformer), batch_size=params.batch_size, shuffle=False,\n",
        "                                    num_workers=params.num_workers,\n",
        "                                    pin_memory=params.cuda)\n",
        "            dataloaders[split] = dl\n",
        "    return dataloaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4drEQEXuaoZY"
      },
      "source": [
        "# Metrics\n",
        "All metrics take in the raw outputs from the model and the labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoPG4nXFasaD"
      },
      "source": [
        "def accuracy(outputs, labels):\n",
        "    outputs = np.rint(outputs)\n",
        "    return accuracy_score(labels, outputs)\n",
        "\n",
        "def recall(outputs, labels):\n",
        "    outputs = np.rint(outputs)\n",
        "    return recall_score(labels, outputs)\n",
        "\n",
        "def precision(outputs, labels):\n",
        "    outputs = np.rint(outputs)\n",
        "    return precision_score(labels, outputs)  \n",
        "\n",
        "def f1(outputs, labels):\n",
        "    outputs = np.rint(outputs)\n",
        "    return f1_score(labels, outputs)  \n",
        "\n",
        "def specificity(outputs, labels):\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, np.rint(outputs), labels=[0,1]).ravel()\n",
        "    return tn / (tn+fp)\n",
        "\n",
        "def cohen_kappa(outputs, labels):\n",
        "    outputs = np.rint(outputs)\n",
        "    return cohen_kappa_score(outputs, labels)\n",
        "\n",
        "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
        "metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    'recall': recall,\n",
        "    'precision': precision,\n",
        "    'f1_score': f1,\n",
        "    'specificity': specificity,\n",
        "    'cohen': cohen_kappa\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsPfS6k3UkZz"
      },
      "source": [
        "# Utility Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gwoNsgTDKrY"
      },
      "source": [
        "# Utils\n",
        "class Params():\n",
        "    \"\"\"Class that loads hyperparameters from a json file.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    params = Params(json_path)\n",
        "    print(params.learning_rate)\n",
        "    params.learning_rate = 0.5  # change the value of learning_rate in params\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, json_path):\n",
        "        with open(json_path) as f:\n",
        "          params = json.load(f)\n",
        "          self.__dict__.update(params)\n",
        "\n",
        "    def save(self, json_path):\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(self.__dict__, f, indent=4)\n",
        "\n",
        "    def update(self, json_path):\n",
        "        \"\"\"Loads parameters from json file\"\"\"\n",
        "        with open(json_path) as f:\n",
        "            params = json.load(f)\n",
        "            self.__dict__.update(params)\n",
        "\n",
        "    @property\n",
        "    def dict(self):\n",
        "        \"\"\"Gives dict-like access to Params instance by `params.dict['learning_rate']\"\"\"\n",
        "        return self.__dict__\n",
        "\n",
        "\n",
        "class RunningAverage():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "\n",
        "    def __call__(self):\n",
        "        return self.total / float(self.steps)\n",
        "\n",
        "\n",
        "def set_logger(log_path):\n",
        "    \"\"\"Set the logger to log info in terminal and file `log_path`.\n",
        "\n",
        "    In general, it is useful to have a logger so that every output to the terminal is saved\n",
        "    in a permanent file. Here we save it to `model_dir/train.log`.\n",
        "\n",
        "    Example:\n",
        "    ```\n",
        "    logging.info(\"Starting training...\")\n",
        "    ```\n",
        "\n",
        "    Args:\n",
        "        log_path: (string) where to log\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    if not logger.handlers:\n",
        "        # Logging to a file\n",
        "        file_handler = logging.FileHandler(log_path)\n",
        "        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n",
        "        logger.addHandler(file_handler)\n",
        "\n",
        "        # Logging to console\n",
        "        stream_handler = logging.StreamHandler()\n",
        "        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n",
        "        logger.addHandler(stream_handler)\n",
        "\n",
        "\n",
        "def save_dict_to_json(d, json_path):\n",
        "    \"\"\"Saves dict of floats in json file\n",
        "\n",
        "    Args:\n",
        "        d: (dict) of float-castable values (np.float, int, float, etc.)\n",
        "        json_path: (string) path to json file\n",
        "    \"\"\"\n",
        "    with open(json_path, 'w') as f:\n",
        "        # We need to convert the values to float for json (it doesn't accept np.array, np.float, )\n",
        "        d = {k: float(v) for k, v in d.items()}\n",
        "        json.dump(d, f, indent=4)\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, checkpoint):\n",
        "    \"\"\"Saves model and training parameters at checkpoint + 'last.pth.tar'. If is_best==True, also saves\n",
        "    checkpoint + 'best.pth.tar'\n",
        "\n",
        "    Args:\n",
        "        state: (dict) contains model's state_dict, may contain other keys such as epoch, optimizer state_dict\n",
        "        is_best: (bool) True if it is the best model seen till now\n",
        "        checkpoint: (string) folder where parameters are to be saved\n",
        "    \"\"\"\n",
        "    filepath = os.path.join(checkpoint, 'last.pth.tar')\n",
        "    if not os.path.exists(checkpoint):\n",
        "        print(\"Checkpoint Directory does not exist! Making directory {}\".format(checkpoint))\n",
        "        os.mkdir(checkpoint)\n",
        "    else:\n",
        "        print(\"Checkpoint Directory exists! \")\n",
        "    torch.save(state, filepath)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filepath, os.path.join(checkpoint, 'best.pth.tar'))\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer=None):\n",
        "    \"\"\"Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n",
        "    optimizer assuming it is present in checkpoint.\n",
        "\n",
        "    Args:\n",
        "        checkpoint: (string) filename which needs to be loaded\n",
        "        model: (torch.nn.Module) model for which the parameters are loaded\n",
        "        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n",
        "    \"\"\"\n",
        "    if not os.path.exists(checkpoint):\n",
        "        raise (\"File doesn't exist {}\".format(checkpoint))\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    if optimizer:\n",
        "        optimizer.load_state_dict(checkpoint['optim_dict'])\n",
        "\n",
        "    return checkpoint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN81GP63Uso0"
      },
      "source": [
        "# Evaluate Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75BJNyo9-kIM"
      },
      "source": [
        "def evaluate(model, loss_fn, dataloader, metrics, params):\n",
        "    \"\"\"Evaluate the model\n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches data\n",
        "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "        params: (Params) hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # summary for current eval loop\n",
        "    summ = []\n",
        "\n",
        "    # compute metrics over the dataset\n",
        "    for data_batch, labels_batch in dataloader:\n",
        "\n",
        "        # reshape labels batch for our loss function\n",
        "        labels_batch = labels_batch.type(torch.FloatTensor).reshape((labels_batch.shape[0], 1))\n",
        "\n",
        "        # move to GPU if available\n",
        "        if params.cuda:\n",
        "            data_batch, labels_batch = data_batch.cuda(\n",
        "                non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
        "        # fetch the next evaluation batch\n",
        "        data_batch, labels_batch = Variable(data_batch), Variable(labels_batch)\n",
        "\n",
        "        # compute model output\n",
        "        output_batch = model(data_batch)\n",
        "        #labels_batch = labels_batch.type(torch.FloatTensor).reshape((output_batch.shape[0], 1)).cuda(non_blocking=True)\n",
        "        loss = loss_fn(output_batch, labels_batch)\n",
        "\n",
        "        # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
        "        output_batch = output_batch.data.cpu().numpy()\n",
        "        labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "        # compute all metrics on this batch\n",
        "        summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
        "                         for metric in metrics}\n",
        "        summary_batch['loss'] = loss.item()\n",
        "        summ.append(summary_batch)\n",
        "\n",
        "    # compute mean of all metrics in summary\n",
        "    metrics_mean = {metric: np.mean([x[metric]\n",
        "                                     for x in summ]) for metric in summ[0]}\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
        "                                for k, v in metrics_mean.items())\n",
        "    logging.info(\"- Eval metrics : \" + metrics_string)\n",
        "    return metrics_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsf8v7uWUyKr"
      },
      "source": [
        "# Training Method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afh7PkedTL79"
      },
      "source": [
        "def train(model, optimizer, loss_fn, dataloader, metrics, params):\n",
        "    \"\"\"Train the model \n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        optimizer: (torch.optim) optimizer for parameters of model\n",
        "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "        dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
        "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "        params: (Params) hyperparameters\n",
        "    \"\"\"\n",
        "\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # summary for current training loop and a running average object for loss\n",
        "    summ = []\n",
        "    loss_avg = RunningAverage()\n",
        "\n",
        "    # Use tqdm for progress bar\n",
        "    with tqdm(total=len(dataloader)) as t:\n",
        "        for i, (train_batch, labels_batch) in enumerate(dataloader):\n",
        "            # reshape labels_batch\n",
        "            labels_batch = labels_batch.type(torch.FloatTensor).reshape((labels_batch.shape[0], 1))\n",
        "\n",
        "            # move to GPU if available\n",
        "            if params.cuda:\n",
        "                train_batch, labels_batch = train_batch.cuda(\n",
        "                    non_blocking=True), labels_batch.cuda(non_blocking=True)\n",
        "            # convert to torch Variables\n",
        "            train_batch, labels_batch = Variable(\n",
        "                train_batch), Variable(labels_batch)\n",
        "\n",
        "            # compute model output and loss\n",
        "            output_batch = model(train_batch)\n",
        "            #labels_batch = labels_batch.type(torch.FloatTensor).reshape((output_batch.shape[0], 1)).cuda(non_blocking=True)\n",
        "            loss = loss_fn(output_batch, labels_batch)\n",
        "\n",
        "            # clear previous gradients, compute gradients of all variables wrt loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # performs updates using calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "            # Evaluate summaries only once in a while\n",
        "            if i % params.save_summary_steps == 0:\n",
        "                # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
        "                output_batch = output_batch.data.cpu().numpy()\n",
        "                labels_batch = labels_batch.data.cpu().numpy()\n",
        "\n",
        "                # compute all metrics on this batch\n",
        "                summary_batch = {metric: metrics[metric](output_batch, labels_batch)\n",
        "                                 for metric in metrics}\n",
        "                summary_batch['loss'] = loss.item()\n",
        "                summ.append(summary_batch)\n",
        "\n",
        "            # update the average loss\n",
        "            loss_avg.update(loss.item())\n",
        "\n",
        "            t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
        "            t.update()\n",
        "\n",
        "    # compute mean of all metrics in summary\n",
        "    metrics_mean = {metric: np.mean([x[metric]\n",
        "                                     for x in summ]) for metric in summ[0]}\n",
        "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v)\n",
        "                                for k, v in metrics_mean.items())\n",
        "    logging.info(\"- Train metrics: \" + metrics_string)\n",
        "    return metrics_mean\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, train_dataloader, val_dataloader, optimizer, loss_fn, metrics, params, model_dir,\n",
        "                       restore_file=None):\n",
        "    \"\"\"Train the model and evaluate every epoch.\n",
        "\n",
        "    Args:\n",
        "        model: (torch.nn.Module) the neural network\n",
        "        train_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches training data\n",
        "        val_dataloader: (DataLoader) a torch.utils.data.DataLoader object that fetches validation data\n",
        "        optimizer: (torch.optim) optimizer for parameters of model\n",
        "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
        "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
        "        params: (Params) hyperparameters\n",
        "        model_dir: (string) directory containing config, weights and log\n",
        "        restore_file: (string) optional- name of file to restore from \n",
        "    \"\"\"\n",
        "    # reload weights from restore_file if specified\n",
        "    if restore_file is not None:\n",
        "        logging.info(\"Restoring parameters from {}\".format(restore_file))\n",
        "        load_checkpoint(restore_file, model, optimizer)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    writer = SummaryWriter(model_dir + \"/tensorboard\")\n",
        "\n",
        "    for epoch in range(params.num_epochs):\n",
        "        # Run one epoch\n",
        "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
        "\n",
        "        # compute number of batches in one epoch (one full pass over the training set)\n",
        "        train_metrics = train(model, optimizer, loss_fn, train_dataloader, metrics, params)\n",
        "\n",
        "        # Evaluate for one epoch on validation set\n",
        "        val_metrics = evaluate(model, loss_fn, val_dataloader, metrics, params)\n",
        "\n",
        "        val_acc = val_metrics['accuracy']\n",
        "        is_best = val_acc >= best_val_acc\n",
        "\n",
        "        # Save weights\n",
        "        save_checkpoint({'epoch': epoch + 1,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optim_dict': optimizer.state_dict()},\n",
        "                        is_best=is_best,\n",
        "                        checkpoint=model_dir)\n",
        "        \n",
        "        # Log the data to tensorboard\n",
        "        for metric_name in train_metrics:\n",
        "          writer.add_scalar(metric_name.title() + \"/train\", train_metrics[metric_name], epoch)\n",
        "          writer.add_scalar(metric_name.title() + \"/validation\", val_metrics[metric_name], epoch)\n",
        "          \n",
        "\n",
        "        # If best_eval, best_save_path\n",
        "        if is_best:\n",
        "            logging.info(\"- Found new best accuracy\")\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "            # Save best val metrics in a json file in the model directory\n",
        "            best_json_path = os.path.join(\n",
        "                model_dir, \"metrics_val_best_weights.json\")\n",
        "            save_dict_to_json(val_metrics, best_json_path)\n",
        "\n",
        "        # Save latest val metrics in a json file in the model directory\n",
        "        last_json_path = os.path.join(\n",
        "            model_dir, \"metrics_val_last_weights.json\")\n",
        "        save_dict_to_json(val_metrics, last_json_path)\n",
        "    \n",
        "    # Close off the SummaryWriter\n",
        "    writer.flush()\n",
        "    writer.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlUzzZ08GgGV"
      },
      "source": [
        "# ResNet-152 Configuration\n",
        "Here, we setup our model, a ResNet-152, initialized with weights pretrained on ImageNet and provided by PyTorch. See https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "\n",
        "We also make some modifications to the final layer, changing into into a sequence of a Dropout layer, a FC layer with one output, and a sigmoid nonlinearity. \n",
        "\n",
        "Hyperparamter tuning found freezing the first 148-layers yields the best performance, so this has been done here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQSCOAhucvl4"
      },
      "source": [
        "def load_model(dropout_rate, restore_file=None):\n",
        "  model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet152', pretrained=True)\n",
        "  model.fc = nn.Sequential(\n",
        "          nn.Dropout2d(p=dropout_rate, inplace=True),\n",
        "          nn.Linear(2048, 1),\n",
        "          nn.Sigmoid()\n",
        "        )\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False \n",
        "  for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "  for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "  if restore_file:\n",
        "    load_checkpoint(restore_file, model)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvkI1s3CU41I"
      },
      "source": [
        "# Setup and Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rG3uOyvNOi-"
      },
      "source": [
        "## Retrieve Dataset\n",
        "The raw MURA Dataset is available here: https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "\n",
        "In a separate notebook, I downloaded the dataset and did some simple preprocessing on it (resizing images and labeling files more nicely). Here, I connect to Google Drive so I can later retrieve the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3BJtYe4Vhdz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# If you wish to download the MURA dataset directly, uncomment the snippet below\n",
        "# !wget https://cs.stanford.edu/group/mlgroup/MURA-v1.1.zip\n",
        "# !unzip MURA-v1.1.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOpxS3cNiRQp"
      },
      "source": [
        "## Tensorboard Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwGdQgiyiVSS"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpFLZfmz_51b"
      },
      "source": [
        "## Driver to Execute Training and Evaluation on Validation Set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geNcJQn9DFKk"
      },
      "source": [
        "def run_model(model_dir, data_dir, restore_file):\n",
        "  # Load the parameters from json file\n",
        "  json_path = os.path.join(model_dir, 'params.json')\n",
        "  assert os.path.isfile(json_path), \"No json configuration file found at {}\".format(json_path)\n",
        "  params = Params(json_path)\n",
        "\n",
        "  # use GPU if available\n",
        "  params.cuda = torch.cuda.is_available()\n",
        "\n",
        "  # Set the random seed for reproducible experiments\n",
        "  torch.manual_seed(230)\n",
        "  if params.cuda:\n",
        "      torch.cuda.manual_seed(230)\n",
        "\n",
        "  # Set the logger\n",
        "  set_logger(os.path.join(model_dir, 'train.log'))\n",
        "\n",
        "  # Create the input data pipeline\n",
        "  logging.info(\"Loading the datasets...\")\n",
        "\n",
        "  # fetch dataloaders\n",
        "  dataloaders = fetch_dataloader(\n",
        "      ['train', 'val'], data_dir, params)\n",
        "  train_dl = dataloaders['train']\n",
        "  val_dl = dataloaders['val']\n",
        "\n",
        "  logging.info(\"- done.\")\n",
        "\n",
        "  # Define the model and optimizer\n",
        "  model = load_model(params.dropout_rate)\n",
        "  model = model.to('cuda') if params.cuda else model\n",
        "  optimizer = optim.Adam(model.parameters(), lr=params.learning_rate, weight_decay=params.weight_decay)\n",
        "\n",
        "  # fetch loss function and metrics\n",
        "  loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "  # Train the model\n",
        "  logging.info(\"Starting training for {} epoch(s)\".format(params.num_epochs))\n",
        "  train_and_evaluate(model, train_dl, val_dl, optimizer, loss_fn, metrics, params, model_dir, restore_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6bm794aOeS2"
      },
      "source": [
        "## Execute Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EucyL1zHWDyq"
      },
      "source": [
        "run_model('/content/drive/My Drive/CS 230/Project/experiments/final', \n",
        "          '/content/drive/My Drive/CS 230/Project/mura',\n",
        "          None)\n",
        "\n",
        "%tensorboard --logdir='/content/drive/My Drive/CS 230/Project/experiments/final'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlogVJMz6-OI"
      },
      "source": [
        "# Test Set Evaluation\n",
        "Here is the code we used for evaluating the performance of our final model on the test set, included here for completeness. \n",
        "\n",
        "We calculated metrics based on its performance on the test set and drilled down further into the different regions of the body represented in the MURA dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnDCMVXE7BIv"
      },
      "source": [
        "def get_all_preds_and_labels(model, dataloader, cuda):\n",
        "  model.eval()\n",
        "  all_preds, all_labels = torch.tensor([]), torch.tensor([])\n",
        "  if cuda:\n",
        "    all_preds, all_labels = torch.tensor([]).cuda(non_blocking=True), torch.tensor([]).cuda(non_blocking=True)\n",
        "    model = model.to('cuda')\n",
        "  with tqdm(total=len(dataloader)) as t:\n",
        "    for batch in dataloader:\n",
        "        images, labels = batch\n",
        "        if cuda:\n",
        "          images, labels = images.cuda(non_blocking=True), labels.cuda(non_blocking=True)\n",
        "\n",
        "        # get predictions\n",
        "        preds = model(images)\n",
        "\n",
        "        # concatenate onto the ongoing list\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "        all_labels = torch.cat(\n",
        "            (all_labels, labels)\n",
        "            ,dim=0\n",
        "        )\n",
        "        t.update()\n",
        "  return all_preds, all_labels\n",
        "\n",
        "def calculate_stats(predictions, labels, metrics):\n",
        "  # compute all metrics on t\n",
        "  summary_batch = {metric: metrics[metric](predictions, labels)\n",
        "                         for metric in metrics}\n",
        "  return summary_batch\n",
        "\n",
        "def plot_confusion_matrix(predictions, labels, title):\n",
        "  # calculate confusion matrix \n",
        "  matrix = confusion_matrix(labels, np.rint(predictions))\n",
        "  \n",
        "  df_cm = pd.DataFrame(matrix, ['negative', 'positive'], ['negative', 'positive'])\n",
        "  \n",
        "  plt.figure(figsize=(10,7))\n",
        "  sn.set(font_scale=1.4) # for label size\n",
        "  sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g') # font size\n",
        "\n",
        "  plt.title(\"Confusion Matrix: \" + title)\n",
        "  plt.ylabel('Actual')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "def plot_auc_roc(predictions, labels):\n",
        "  fpr, tpr, threshold = roc_curve(labels, predictions)\n",
        "  roc_auc = auc(fpr, tpr)\n",
        "\n",
        "  plt.title('Receiver Operating Characteristic')\n",
        "  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "  plt.legend(loc = 'lower right')\n",
        "  plt.plot([0, 1], [0, 1],'r--')\n",
        "  plt.xlim([0, 1])\n",
        "  plt.ylim([0, 1])\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def get_stats(model, data_dir, params, metrics):\n",
        "  dataloaders = fetch_dataloader(['test'], data_dir, params)\n",
        "  predictions, labels = get_all_preds_and_labels(model, dataloaders['test'], params.cuda)\n",
        "\n",
        "  predictions = predictions.data.cpu().numpy()\n",
        "  labels = labels.data.cpu().numpy()\n",
        "\n",
        "  # do the confusion matrix\n",
        "  plot_confusion_matrix(predictions, labels, \"Overall\")\n",
        "\n",
        "  # do the ROC curve\n",
        "  plot_auc_roc(predictions, labels)\n",
        "\n",
        "  # print overall metrics\n",
        "  print(\"Overall\")\n",
        "  print(calculate_stats(predictions, labels, metrics))\n",
        "\n",
        "  for region in Region:\n",
        "    predictions, labels = get_all_preds_and_labels(model, dataloaders[region.name], params.cuda)\n",
        "\n",
        "    predictions = predictions.data.cpu().numpy()\n",
        "    labels = labels.data.cpu().numpy()\n",
        "\n",
        "    print(region.name)\n",
        "    print(calculate_stats(predictions, labels, metrics))\n",
        "\n",
        "\n",
        "params = Params('/content/drive/My Drive/CS 230/Project/experiments/final/params.json')\n",
        "params.cuda = torch.cuda.is_available()\n",
        "get_stats(load_model(0.5, \"/content/drive/My Drive/CS 230/Project/experiments/final/best.pth.tar\"), \n",
        "               '/content/drive/My Drive/CS 230/Project/mura', \n",
        "               params,\n",
        "               metrics)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}